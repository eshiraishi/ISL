{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    p(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\\\ \\frac {p(X)}{1-p(X)} = e^{\\beta_0 + \\beta_1 X} \\tag{odds}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\\\ \\log{\\frac {p(X)}{1-p(X)}} = \\beta_0 + \\beta_1 X \\tag{logit}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muitas funções não lineares podem ser otimizadas corretamente maximizando $\\ell$, e tecnicamente os mínimos quadrados são um caso especial de $\\ell$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\ell(\\beta_0, \\beta_1) = \\prod_{i:y_i=1} p(x_i) \\prod_{i':y_{i'}=0}p(x_{i'}) \\tag{Maximum Likelihood}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximizar $\\ell$ significa encontrar coeficientes que façam com que a probabilidade predita tenha a maior correspondência com o valor original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de Classificação Generativos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abordagem de predição de classes que usa o teorema de Bayes para determinar as probabilidades de cada valor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais performáticos quando:\n",
    "\n",
    "* Há uma separação alta entre as duas classes.\n",
    "* A distribuição dos preditores é aproximadamente normal em cada classe e há poucas amostras.\n",
    "\n",
    "Além disso pode ser facilmente transformado em uma regressão com mais de 2 classes.\n",
    "\n",
    "Os modelos generativos tentam aproximar os valores usando uma função de densidade de probabilidade estimada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumindo que existe apenas uma variável preditiva e que a distribuição é normal a densidade será:\n",
    "$$\n",
    "    f_k(x) = \\frac 1 {\\sqrt{2 \\pi}\\sigma_k} \\exp\\left(-\\frac 1 {2\\sigma^2_k}(x-\\mu_k)^2\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    p_k(x) = \\frac {\n",
    "        \\pi_k \\frac 1 {\\sqrt{2 \\pi} \\sigma}\\exp({-\\frac 1 {2\\sigma^2}(x-\\mu_k)^2})\n",
    "    } {\n",
    "        \\sigma^K_l=1 \\pi_l \\frac 1 {\\sqrt{2\\pi}\\sigma} \\exp{(-\\frac 1 {2\\sigma^2}(x-\\mu_l)^2)}\n",
    "    }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\pi_k: \\text{Probabilidade de uma observação pertencer à classe k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\delta_k (x) = x \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu^2_k}{2\\sigma^2} + \\log{\\pi_k}\n",
    "$$\n",
    "\n",
    "Onde $\\delta_k$ é a probabilidade de um exemplo pertencer a classe k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "    \\hat \\mu_k = \\frac 1 {n_k} \\sum_{i:y_i=k} x_i \\\\\n",
    "    \\hat \\sigma^2 = \\frac 1 {n-K}\\sum^{K}_{k=1} \\sum_{i:y_i=k}(x_i - \\hat \\mu_k)^2 \\\\\n",
    "    n_k: \\text{total de exemplos de treino na classe k} \\\\\n",
    "    \\hat \\pi_k = \\frac {n_k}{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando existem mais de 2 preditores:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    f(x) = \\frac 1 {(2\\pi)^{\\frac p 2}|\\Sigma|^{\\frac 1 2}} \\exp{\\left(-\\frac 1 2 (x-\\mu)^T \\Sigma^{-1}(x-\\mu)\\right)}\n",
    "$$\n",
    "\n",
    "Onde $\\Sigma$ é a matriz de covariância.\n",
    "\n",
    "$$\n",
    "    \\delta_k(x) = x^T \\Sigma^{-1}\\mu_k - \\frac 1 2 \\mu^T_k\\Sigma^{-1} \\mu_k - \\log \\pi_k\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curva ROC\n",
    "\n",
    "O nome dessa curva é histórico e vem da teoria das comunicações (não tem a ver com classificação em si).\n",
    "\n",
    "Quanto mais espaço no gráfico a área abaixo da curva ocupar, maior o desempenho do LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipos de Erro em classificação\n",
    "* Tipo 1: Falso positivo. Métrica: especificidade\n",
    "* Tipo 2: Falso negativo. Métrica: Poder, sensitividade, recall\n",
    "* Tipo 3: Verdadeiros positivos. Proporção de falsas descobertas. Métrica: Precisão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Discriminant Analysis\n",
    "\n",
    "Assume que a distribuição dos dados é normal mas assume que cada classe tem sua própria matriz de covariância."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
