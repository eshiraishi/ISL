{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    p(X) = \\frac{\n",
    "        e^{\\beta_0 + \\beta_1 X}\n",
    "    } {\n",
    "        1 + e^{\\beta_0 + \\beta_1 X}\n",
    "    }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\\\ \\log{\\frac {p(X)}{1-p(X)}} = \\beta_0 + \\beta_1 X \\tag{logit}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muitas funções não lineares podem ser otimizadas corretamente maximizando $\\ell$, e tecnicamente os mínimos quadrados são um caso especial de $\\ell$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\ell(\\beta_0, \\beta_1) = \\prod_{i:y_i=1} p(x_i) \\prod_{i':y_{i'}=0}p(x_{i'}) \\tag{Maximum Likelihood}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximizar $\\ell$ significa encontrar coeficientes que façam com que a probabilidade predita tenha a maior correspondência com o valor original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de Classificação Generativos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abordagem de predição de classes que usa o teorema de Bayes para determinar as probabilidades de cada valor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais performáticos quando:\n",
    "\n",
    "* Há uma separação alta entre as duas classes.\n",
    "* A distribuição dos preditores é aproximadamente normal em cada classe e há poucas amostras.\n",
    "\n",
    "Além disso pode ser facilmente transformado em uma regressão com mais de 2 classes.\n",
    "\n",
    "Os modelos generativos tentam aproximar os valores usando uma função de densidade de probabilidade estimada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumindo que existe apenas uma variável preditiva e que a distribuição é normal a densidade será:\n",
    "$$\n",
    "    f_k(x) = \\frac 1 {\\sqrt{2 \\pi}\\sigma_k} \\exp\\left(-\\frac 1 {2\\sigma^2_k}(x-\\mu_k)^2\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\\\ \\frac {p(X)}{1-p(X)} = e^{\\beta_0 + \\beta_1 X} \\tag{odds}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    p_k(x) = \\frac {\n",
    "        \\pi_k \\frac 1 {\\sqrt{2 \\pi} \\sigma}\\exp({-\\frac 1 {2\\sigma^2}(x-\\mu_k)^2})\n",
    "    } {\n",
    "        \\sigma^K_l=1 \\pi_l \\frac 1 {\\sqrt{2\\pi}\\sigma} \\exp{(-\\frac 1 {2\\sigma^2}(x-\\mu_l)^2)}\n",
    "    }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\pi_k: \\text{Probabilidade de uma observação pertencer à classe k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\delta_k (x) = x \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu^2_k}{2\\sigma^2} + \\log{\\pi_k}\n",
    "$$\n",
    "\n",
    "Onde $\\delta_k$ é a probabilidade de um exemplo pertencer a classe k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "    \\hat \\mu_k = \\frac 1 {n_k} \\sum_{i:y_i=k} x_i \\\\\n",
    "    \\hat \\sigma^2 = \\frac 1 {n-K}\\sum^{K}_{k=1} \\sum_{i:y_i=k}(x_i - \\hat \\mu_k)^2 \\\\\n",
    "    n_k: \\text{total de exemplos de treino na classe k} \\\\\n",
    "    \\hat \\pi_k = \\frac {n_k}{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando existem mais de 2 preditores:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    f(x) = \\frac 1 {(2\\pi)^{\\frac p 2}|\\Sigma|^{\\frac 1 2}} \\exp{\\left(-\\frac 1 2 (x-\\mu)^T \\Sigma^{-1}(x-\\mu)\\right)}\n",
    "$$\n",
    "\n",
    "Onde $\\Sigma$ é a matriz de covariância.\n",
    "\n",
    "$$\n",
    "    \\delta_k(x) = x^T \\Sigma^{-1}\\mu_k - \\frac 1 2 \\mu^T_k\\Sigma^{-1} \\mu_k - \\log \\pi_k\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curva ROC\n",
    "\n",
    "O nome dessa curva é histórico e vem da teoria das comunicações (não tem a ver com classificação em si).\n",
    "\n",
    "Quanto mais espaço no gráfico a área abaixo da curva ocupar, maior o desempenho do LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipos de Erro em classificação\n",
    "* Tipo 1: Falso positivo. Métrica: especificidade\n",
    "* Tipo 2: Falso negativo. Métrica: Poder, sensitividade, recall\n",
    "* Tipo 3: Verdadeiros positivos. Proporção de falsas descobertas. Métrica: Precisão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Discriminant Analysis\n",
    "\n",
    "Assume que a distribuição dos dados é normal mas assume que cada classe tem sua própria matriz de covariância.\n",
    "\n",
    "Isso faz com que o número de parâmetros a serem estimados suba de $p(p+1)/2$ para $Kp(p+1)/2$, ou seja, o modelo é muito mais flexível. Ou seja, a escolha entre QDA e LDA depende do dilema viés variância."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "Assume que todos os preditores são independentes (não partem de uma mesma distribuição), logo não é necessário determinar a distribuição conjunta dos preditores.\n",
    "\n",
    "$$\n",
    "    f_k(x) = \\prod_{i=1}^n f_{k_i}(x_i) \\\\\n",
    "    Pr(Y=k|X=x) = \\frac{\\pi_k \\times f_k(x)}{\\pi_l \\times \\sum^K_{l=1} f_l(x)}\n",
    "$$\n",
    "\n",
    "Formas de estimar essa função:\n",
    "\n",
    "* Se X é quantitativo, assumir que cada distribuição dos preditores é normal (mas não possuem interação entre si)\n",
    "* Se X é quantitativo, estimar de forma não paramétrica (usando um histograma)\n",
    "* Se X é quantitativo, usar um estimador de densidade do kernel (um histograma suavizado)\n",
    "* Se X é qualitativo, contar a proporção de cada classe no dataset e atribuir a probabilidade à proporção."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observações\n",
    "\n",
    "* LDA se chama assim porque o logit dessa função é linear\n",
    "* QDA se chama assim porque o logit dessa função é quadrático\n",
    "* Naive Bayes é um caso específico de LDA\n",
    "* QDA é mais flexível que LDA\n",
    "* Naive Bayes é mais flexível que QDA\n",
    "* KNN é mais flexível que Naive Bayes"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
